{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f5365a",
   "metadata": {},
   "source": [
    "# Diferenças entre Adaboost e GBM\n",
    "\n",
    "## 1\n",
    "No Adaboost, é uma floresta de Stumps, que são classificações que sempre tem 1 de profundidade e no máximo 2 folhas, já no GBM são uma floresta de Árvores, que podem ter diversos folhas e maior profundidade.\n",
    "\n",
    "## 2\n",
    "No Adaboost o primeiro passo é um Stump, e no GBM, o primeiro passo é a média do GBM, fazendo as árvores de decisão a partir do resíduo.\n",
    "\n",
    "## 3\n",
    "No Adaboost, são considerados todas árvores para a classificação/resultado e isso é atribuído ponderadamente por pesos, para considerar apenas os modelos/stumps considerados mais úteis, já no GBM são consideradas todas as árvores que possuem diferentes resultados porém todas são multiplicadas comumente pelo \"learning_rate\" (eta).\n",
    "\n",
    "## 4\n",
    "o Adaboost é mais sensível a Outliers por usar \"logit\" como função de perda padrão, já no GBM podem ser usadas outros tipos de funções de perda como MSE, MAE, log loss, etc.\n",
    "\n",
    "## 5\n",
    "O AdaBoost foi originalmente desenvolvido para classificação binária, pois ele foca apenas em saber se a amostra foi classificada corretamente ou não, ajustando os pesos das amostras erradas. Já o GBM é mais flexível, pois usa o erro residual e permite otimizar diferentes funções de perda, o que o torna aplicável tanto a classificação binária e multiclasse quanto a regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052c970",
   "metadata": {},
   "source": [
    "# Código de Exemplo Execução GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd25dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c7d1d",
   "metadata": {},
   "source": [
    "## Hyperparâmetros GBM\n",
    "1 - n_estimators: é o número de weak learners que serão criados.  \n",
    "2 - learning_rate (eta): é o quanto cada weak learner vai contribuir para a combinação final.  \n",
    "3 - loss: é o hiperparâmetro que define a função de perda a ser otimizada durante o treinamento. Ele determina como os erros entre as previsões e os valores reais serão penalizados e corrigidos a cada iteração.  \n",
    "4 - max_depth: dependendo do tipo de modelo pode ser usado, é a profundidade.  \n",
    "5 - min_samples_leaf: dependendo do tipo de modelo pode ser usado, para ter um número mínimo de folhas.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
